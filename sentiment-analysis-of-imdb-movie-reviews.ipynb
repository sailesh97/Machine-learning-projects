{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMDB Dataset.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "4c593c17588723c0b0b0f19851cb70a8447ced76",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data=pd.read_csv('../input/IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7f11c83b1320c8982b36889145f7f770563674a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "cb6bb97b0f851947dcf341a1de5708a1f2bc64c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    25000\n",
       "positive    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d3aaabff555e07feb11c72cc3a6e457615975ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (40000,)\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#IMDB dataset\n",
    "reviews=np.array(imdb_data['review'])\n",
    "sentiments=np.array(imdb_data['sentiment'])\n",
    " #Split the dataset\n",
    "train_reviews=reviews[:40000]\n",
    "train_sentiments=sentiments[:40000]\n",
    "test_reviews=reviews[40000:]\n",
    "test_sentiments=sentiments[40000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "5cb57ee6f7087b9ae0592c01a220a638955c2137",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"won't\", 'against', 'couldn', 'me', 'shouldn', 'and', \"mightn't\", 'should', 'but', 'again', 'hadn', \"you'd\", 'needn', 'were', 't', 'own', 'this', 'himself', 'there', 'or', 'before', 'no', 'once', 'the', 'because', 'then', \"should've\", 'them', 'is', 'yourself', 'been', 'ain', \"weren't\", 'at', 'who', 'ourselves', 'out', 'mustn', 's', 'yourselves', 'under', 'my', 'will', 'ma', 'was', 'those', 'just', \"hadn't\", 'when', \"needn't\", 'any', 'having', \"isn't\", 'hers', 'd', 'itself', 'an', \"it's\", 'more', 'with', 'such', 'both', 'than', 'mightn', 'll', 'very', 'between', 'shan', \"wouldn't\", 'did', 'won', 'they', 'about', 'all', 'down', \"doesn't\", 'its', 've', 'other', 'him', 'didn', 'herself', 'in', 'now', 'doing', 'into', \"couldn't\", 'through', 'aren', 'further', 'does', \"hasn't\", 'isn', 'yours', 'until', 'on', 'during', 'which', 'wasn', 'm', 'a', 'nor', 'so', 'for', \"you'll\", 'above', 'it', 'where', 'their', 'after', 'most', \"that'll\", 're', 'have', 'why', 'haven', 'he', 'too', 'weren', \"she's\", 'as', 'of', 'don', \"shouldn't\", 'myself', 'ours', \"you're\", 'am', 'off', 'only', \"haven't\", 'how', 'o', 'not', 'if', 'she', 'being', 'his', 'while', 'below', 'themselves', 'over', \"mustn't\", \"didn't\", 'hasn', 'to', \"aren't\", 'whom', 'theirs', 'few', 'had', 'has', 'y', 'we', 'that', 'i', 'you', \"wasn't\", 'these', 'same', \"you've\", 'our', 'do', 'what', \"shan't\", 'doesn', 'here', 'wouldn', 'up', 'some', 'can', 'by', 'her', 'from', 'each', \"don't\", 'be', 'your', 'are'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'one review ha mention watch 1 Oz episod hook right thi exactli happen mebr br first thing struck Oz wa brutal unflinch scene violenc set right word GO trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordbr br call OZ nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda Em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awaybr br would say main appeal show due fact goe show would dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doe mess around first episod ever saw struck nasti wa surreal could say wa readi watch develop tast Oz got accustom high level graphic violenc violenc injustic crook guard sold nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch Oz may becom comfort uncomfort viewingthat get touch darker side'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text preprocessing or text normalization\n",
    "#Noise removal\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def strip_html(review):\n",
    "    soup=BeautifulSoup(review,\"html.parser\")\n",
    "    return soup.get_text\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "        \n",
    "def denoise_text(text):\n",
    "    trxt=strip_html(text)\n",
    "    text=remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)\n",
    "#remove accented characters\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_accented_chars)\n",
    "\n",
    "CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } \n",
    "#Expand contractions\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "imdb_data['review']=imdb_data['review'].apply(expand_contractions)\n",
    "\n",
    "#remove special characters\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)\n",
    "\n",
    "#Text stemming\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)\n",
    "\n",
    "#Remove stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)\n",
    "imdb_data.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b20c242bd091929ca896ea2c6e936ca00efe6ecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one review ha mention watch 1 Oz episod hook r...\n",
       "1        wonder littl product br br film techniqu veri ...\n",
       "2        thought thi wa wonder way spend time hot summe...\n",
       "3        basic famili littl boy jake think zombi hi clo...\n",
       "4        petter mattei love time money visual stun film...\n",
       "5        probabl alltim favorit movi stori selfless sac...\n",
       "6        sure would like see resurrect date seahunt ser...\n",
       "7        thi show wa amaz fresh innov idea 70 first air...\n",
       "8        encourag posit comment thi film wa look forwar...\n",
       "9        like origin gut wrench laughter like thi movi ...\n",
       "10       phil alien one quirki film humour base around ...\n",
       "11       saw thi movi wa 12 came recal scariest scene w...\n",
       "12       im big fan boll work mani enjoy hi movi postal...\n",
       "13       cast play shakespearebr br shakespear lostbr b...\n",
       "14       thi fantast movi three prison becom famou one ...\n",
       "15       kind drawn erot scene onli realiz thi wa one a...\n",
       "16       film simpli remad thi one bad film fail captur...\n",
       "17       thi movi made one top 10 aw movi horribl br br...\n",
       "18       rememb thi filmit wa first film watch cinema p...\n",
       "19       aw film must real stinker nomin golden globe t...\n",
       "20       success die hard sequel surpris realli 1990 gl...\n",
       "21       terribl misfortun view thi bmovi entiretybr br...\n",
       "22       absolut stun movi 25 hr kill watch regret much...\n",
       "23       first let us get thing straight anim fan alway...\n",
       "24       thi wa worst movi saw worldfest also receiv le...\n",
       "25       karen carpent stori show littl singer karen ca...\n",
       "26       cell exot masterpiec dizzi trip onli vast mind...\n",
       "27       thi film tri mani thing onc sting polit satir ...\n",
       "28       thi movi wa frustrat everyth seem energet wa t...\n",
       "29       war movi hollywood genr ha done redon mani tim...\n",
       "                               ...                        \n",
       "39970    proper place file thi crap sorri mysteri claim...\n",
       "39971    hayao miyazaki ha captur imagin audienc young ...\n",
       "39972    anoth fantast film countri due decad oppress f...\n",
       "39973    surpris mani peopl give thi move less 7 starsb...\n",
       "39974    uta hagen respect act standard textbook mani c...\n",
       "39975    thi first must see film seen last year wickedl...\n",
       "39976    thi realli silli job miscastingabout bad hepbu...\n",
       "39977    chucki 1 wa good chucki 2 wa better chucki 3 s...\n",
       "39978    smell like garbag look like garbag must garbag...\n",
       "39979    tdi probabl singl worst piec trash ever hit st...\n",
       "39980    thi come close worst movi ever seen writer sta...\n",
       "39981    silli movi look nice doe make lot sens one han...\n",
       "39982    movi onli enter cinema indonesia thi year 2007...\n",
       "39983    cannot believ sat thi utter wast time wa fasci...\n",
       "39984    sure whi articl poster anyth thi film becaus s...\n",
       "39985    wonder whi anymor mr murphi movi thi one becau...\n",
       "39986    clearli see whi robin hood flop quickli first ...\n",
       "39987    director thi movi famou french TV present patr...\n",
       "39988    anyon actual abil sit thi movi walk away feel ...\n",
       "39989    tri rememb name thi movi year consecut cours s...\n",
       "39990    two new oss 117 movi ha kind humor intellig du...\n",
       "39991    thi wa well written tale make batman sitcom ac...\n",
       "39992    approach thi film low expect wa veri pleasantl...\n",
       "39993    wa die see thi onc saw ridicul meatbal poster ...\n",
       "39994    famili guy ha time favorit cartoonit definit f...\n",
       "39995    thi wa marvel funni comedi great cast john rit...\n",
       "39996    plot central charact move camera closeup fact ...\n",
       "39997    thi show awesom love actor ha great stori line...\n",
       "39998    fact thi movi ha entitl success movi switzerla...\n",
       "39999    confess sever disappointedbr br thi version wa...\n",
       "Name: review, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the  dataset\n",
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c5d0d38bd9976150367e9d75f3b933774c96a1ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000    first want say lean liber polit scale found mo...\n",
       "40001    wa excit see sitcom would hope repres indian c...\n",
       "40002    look cover read stuff entir differ type movi c...\n",
       "40003    like mani count appear denni hopper make thi c...\n",
       "40004    thi movi wa tv day enjoy first georg jungl wa ...\n",
       "40005    hickori dickori dock wa good poirot mysteri co...\n",
       "40006    one crazi summer funniest craziest necessarili...\n",
       "40007    lowbudget schlockmeist herschel gordon lewi re...\n",
       "40008    thi wa absolut tragic pile cinema ever born wi...\n",
       "40009    never understood appeal thi show act poor debr...\n",
       "40010    Mr bug goe town wa last major achiev fleischer...\n",
       "40011    thi one two three favorit stoog short undoubte...\n",
       "40012    thi suppos base wilki collin _the_woman_in_whi...\n",
       "40013    british imperialist movi like four feather cha...\n",
       "40014    jame cagney racket polit ward heeler get becom...\n",
       "40015    love thi film swoon Ed wood Jr fan prefer appr...\n",
       "40016    alreadi seen origin jack frost never thought j...\n",
       "40017    subject thi movi relationship class school fig...\n",
       "40018    thi movi stuf full stock horror movi goodi cha...\n",
       "40019    time finish film one wish refund time spent th...\n",
       "40020    anoth film punish us crime enjoy pulp fictionb...\n",
       "40021    romulan come bear gift bring war war conquest ...\n",
       "40022    thi wa movi heard life grow never seen year ag...\n",
       "40023    let begin say bigger fan origin lonesom dove p...\n",
       "40024    wa realli interest see step friend kept bug bu...\n",
       "40025    seen bsg tri watch onc middl show could get ho...\n",
       "40026    well turn expect visual overload noth els ad o...\n",
       "40027    cooley high wa actual drama moment comedi wa r...\n",
       "40028    thi great movi ani fan hong kong action movi a...\n",
       "40029    tri understand peopl like mirrormask avid film...\n",
       "                               ...                        \n",
       "49970    thi movi total dog found strain find anyth lau...\n",
       "49971    sever name actor lanc henrikson david warner j...\n",
       "49972    futur fantasi never look dark christoph lamber...\n",
       "49973    titl lead viewer believ thi fun movi watch pro...\n",
       "49974    part michael disast ten minut charm nineti wor...\n",
       "49975    90 minut mindymindi teas boyfriend billmindi p...\n",
       "49976    saw movi theater releas watch vh tape year str...\n",
       "49977    dog bite dog go everyon realli enjoy full slap...\n",
       "49978    halloween one movi get skin deep opinion scari...\n",
       "49979    saw thi high expect come akshay kumar govinda ...\n",
       "49980    stun film high qualitybr br appar base true ev...\n",
       "49981    repeat pleas see thi movi thi review thi warn ...\n",
       "49982    hone use like thi show watch regularli thank g...\n",
       "49983    love fan origin seri alway wonder back stori w...\n",
       "49984    hello derrick cannon welcom first ever cannoni...\n",
       "49985    imaginari hero clearli best film year wa compl...\n",
       "49986    thi movi disgrac major leagu franchis live min...\n",
       "49987    remak alejandro amenabar abr lo ojo thi time l...\n",
       "49988    first tune thi morn news thought wow final ent...\n",
       "49989    got thi one week ago love modern light fill tr...\n",
       "49990    lame lame lame 90minut cringefest 89 minut lon...\n",
       "49991    le visiteur first movi mediev time travel wa a...\n",
       "49992    john garfield play marin blind grenad fight gu...\n",
       "49993    robert colomb ha two fulltim job known through...\n",
       "49994    thi typic junk comedybr br almost laugh genuin...\n",
       "49995    thought thi movi right good job wa creativ ori...\n",
       "49996    bad plot bad dialogu bad act idiot direct anno...\n",
       "49997    cathol taught parochi elementari school nun ta...\n",
       "49998    go disagre previou comment side maltin thi one...\n",
       "49999    one expect star trek movi high art fan expect ...\n",
       "Name: review, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalization of test reviews\n",
    "norm_test_reviews=imdb_data.review[40000:]\n",
    "norm_test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "35cf9dcefb40b2dc520c5b0d559695324c46cc04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (40000, 1836705)\n",
      "BOW_cv_test: (10000, 1836705)\n"
     ]
    }
   ],
   "source": [
    "#Bags of words of model\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,2))\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n",
    "#vocab=cv.get_feature_names()-toget feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "afe6de957339921e05a6faeaf731f2272fd31946",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (40000, 1836705)\n",
      "Tfidf_test: (10000, 1836705)\n"
     ]
    }
   ],
   "source": [
    "#TfidfVectorizer model\n",
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,2),sublinear_tf=True)\n",
    "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
    "tv_test_reviews=tv.transform(norm_test_reviews)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "a36c058e834938559b7202f2142e61423a613b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.65      0.78      0.71      4993\n",
      "    Negative       0.72      0.58      0.64      5007\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     10000\n",
      "   macro avg       0.69      0.68      0.67     10000\n",
      "weighted avg       0.69      0.68      0.67     10000\n",
      "\n",
      "[[2890 2117]\n",
      " [1104 3889]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb=LabelBinarizer()\n",
    "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)\n",
    "\n",
    "#split the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]\n",
    "\n",
    "#Supervised learning models\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "lr=LogisticRegression(penalty='l2',max_iter=100,C=1,random_state=0)\n",
    "\n",
    "svm=SGDClassifier(loss='hinge',n_iter=100,random_state=0) # SGD(Stochastic Gradient Descent)\n",
    "#Loggistic regression for BOW model\n",
    "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
    "print(lr_bow)\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
    "print(lr_bow_report)\n",
    "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#From the confusion matrix,as we can predict that out of 4993 reviews, 2890 reviews are correctly predicted as positive reviews,\n",
    "#and out of 5007 reviews,3889 reviews are correctly predicted as negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "fa37d39d1cd4dbae92a03550bac04a692f5c93be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.69      0.69      0.69      4993\n",
      "    negative       0.69      0.69      0.69      5007\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     10000\n",
      "   macro avg       0.69      0.69      0.69     10000\n",
      "weighted avg       0.69      0.69      0.69     10000\n",
      "\n",
      "[[3432 1561]\n",
      " [1572 3435]]\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression for Tfidf model\n",
    "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
    "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
    "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['positive','negative'])\n",
    "print(lr_tfidf_report)\n",
    "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict)\n",
    "print(cm_tfidf)\n",
    "#From the confusion matrix,as we can predict that out of 4993 reviews, 3432 reviews are correctly predicted as positive reviews,\n",
    "#and out of 5007 reviews,3435 reviews are correctly predicted as negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "25b3801a56ef7ae734fd46275ec3b25301527c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.65      0.73      0.69      4993\n",
      "    Negative       0.69      0.62      0.65      5007\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     10000\n",
      "   macro avg       0.67      0.67      0.67     10000\n",
      "weighted avg       0.67      0.67      0.67     10000\n",
      " [[3631 1362]\n",
      " [1917 3090]]\n"
     ]
    }
   ],
   "source": [
    "#SVM for BOW model\n",
    "svm_bow=svm.fit(cv_train_reviews,train_sentiments)\n",
    "svm_bow_pred=svm.predict(cv_test_reviews)\n",
    "svm_bow_report=classification_report(test_sentiments,svm_bow_pred,target_names=['Positive','Negative'])\n",
    "cm_bow=confusion_matrix(test_sentiments,svm_bow_pred)\n",
    "print(svm_bow_report,cm_bow)\n",
    "#From the confusion matrix,as we can predict that out of 4993 reviews,3631 reviews are correctly predicted as positive reviews,\n",
    "#and out of 5007 reviews,3090 reviews are correctly predicted as negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "fb19b2f0335fcafc142e0fca86c9fba49a3d95e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.50      1.00      0.67      4993\n",
      "    Negative       0.00      0.00      0.00      5007\n",
      "\n",
      "   micro avg       0.50      0.50      0.50     10000\n",
      "   macro avg       0.25      0.50      0.33     10000\n",
      "weighted avg       0.25      0.50      0.33     10000\n",
      " [[4993    0]\n",
      " [5007    0]]\n"
     ]
    }
   ],
   "source": [
    "#SVM for Tfidf model\n",
    "svm_tf=svm.fit(tv_train_reviews,train_sentiments)\n",
    "svm_tf_pred=svm.predict(tv_test_reviews)\n",
    "svm_tf_report=classification_report(test_sentiments,svm_tf_pred,target_names=['Positive','Negative'])\n",
    "cm_tf=confusion_matrix(test_sentiments,svm_tf_pred)\n",
    "print(svm_tf_report, cm_tf)\n",
    "#From the confusion matrix,as we can predict that out of 4993 reviews, 4993 reviews are correctly predicted as positive reviews,\n",
    "#and out of 5007 reviews,0 reviews are correctly predicted as negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "d6ac75b6c05e11eccee051022d79effd9fa3439d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
